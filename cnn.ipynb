{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.1+cu111'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f78c8e55b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.FashionMNIST(\n",
    "        root = './data',\n",
    "        download = True,\n",
    "        train = True,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "        root = './data',\n",
    "        download = True,\n",
    "        train = False,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "# Hyper Parameters\n",
    "# EPOCH = 3               # train the training data n times, to save time, we just train 1 epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programs\\envs\\python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:64: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "d:\\programs\\envs\\python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:54: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "d:\\programs\\envs\\python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:69: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "d:\\programs\\envs\\python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:59: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot one example\n",
    "print(train_data.train_data.size()),print(train_data.train_labels.size())\n",
    "print(test_data.test_data.size()),print(train_data.test_labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.train_labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"261.959875pt\" version=\"1.1\" viewBox=\"0 0 248.065 261.959875\" width=\"248.065pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2022-05-26T14:20:25.261184</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 261.959875 \r\nL 248.065 261.959875 \r\nL 248.065 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 23.425 241.58175 \r\nL 240.865 241.58175 \r\nL 240.865 24.14175 \r\nL 23.425 24.14175 \r\nz\r\n\" style=\"fill:#eeeeee;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p72d891894d)\">\r\n    <image height=\"218\" id=\"imagecb06829529\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"23.425\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-23.58175\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <path clip-path=\"url(#p72d891894d)\" d=\"M 27.307857 241.58175 \r\nL 27.307857 24.14175 \r\n\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n     </g>\r\n     <g id=\"line2d_2\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 -3.5 \r\n\" id=\"m1f56b284b1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"27.307857\" xlink:href=\"#m1f56b284b1\" y=\"241.58175\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(24.126607 252.680188)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_3\">\r\n      <path clip-path=\"url(#p72d891894d)\" d=\"M 66.136429 241.58175 \r\nL 66.136429 24.14175 \r\n\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n     </g>\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.136429\" xlink:href=\"#m1f56b284b1\" y=\"241.58175\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(62.955179 252.680188)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_5\">\r\n      <path clip-path=\"url(#p72d891894d)\" d=\"M 104.965 241.58175 \r\nL 104.965 24.14175 \r\n\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n     </g>\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"104.965\" xlink:href=\"#m1f56b284b1\" y=\"241.58175\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(98.6025 252.680188)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_7\">\r\n      <path clip-path=\"url(#p72d891894d)\" d=\"M 143.793571 241.58175 \r\nL 143.793571 24.14175 \r\n\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n     </g>\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"143.793571\" xlink:href=\"#m1f56b284b1\" y=\"241.58175\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(137.431071 252.680188)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_9\">\r\n      <path clip-path=\"url(#p72d891894d)\" d=\"M 182.622143 241.58175 \r\nL 182.622143 24.14175 \r\n\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n     </g>\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.622143\" xlink:href=\"#m1f56b284b1\" y=\"241.58175\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(176.259643 252.680188)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_11\">\r\n      <path clip-path=\"url(#p72d891894d)\" d=\"M 221.450714 241.58175 \r\nL 221.450714 24.14175 \r\n\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n     </g>\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"221.450714\" xlink:href=\"#m1f56b284b1\" y=\"241.58175\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(215.088214 252.680188)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_13\">\r\n      <path clip-path=\"url(#p72d891894d)\" d=\"M 23.425 28.024607 \r\nL 240.865 28.024607 \r\n\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n     </g>\r\n     <g id=\"line2d_14\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 3.5 0 \r\n\" id=\"m6a1bba78c1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"23.425\" xlink:href=\"#m6a1bba78c1\" y=\"28.024607\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 31.823826)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_15\">\r\n      <path clip-path=\"url(#p72d891894d)\" d=\"M 23.425 66.853179 \r\nL 240.865 66.853179 \r\n\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n     </g>\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"23.425\" xlink:href=\"#m6a1bba78c1\" y=\"66.853179\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 70.652397)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_17\">\r\n      <path clip-path=\"url(#p72d891894d)\" d=\"M 23.425 105.68175 \r\nL 240.865 105.68175 \r\n\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n     </g>\r\n     <g id=\"line2d_18\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"23.425\" xlink:href=\"#m6a1bba78c1\" y=\"105.68175\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 109.480969)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_19\">\r\n      <path clip-path=\"url(#p72d891894d)\" d=\"M 23.425 144.510321 \r\nL 240.865 144.510321 \r\n\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n     </g>\r\n     <g id=\"line2d_20\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"23.425\" xlink:href=\"#m6a1bba78c1\" y=\"144.510321\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 148.30954)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_21\">\r\n      <path clip-path=\"url(#p72d891894d)\" d=\"M 23.425 183.338893 \r\nL 240.865 183.338893 \r\n\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n     </g>\r\n     <g id=\"line2d_22\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"23.425\" xlink:href=\"#m6a1bba78c1\" y=\"183.338893\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 187.138112)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_23\">\r\n      <path clip-path=\"url(#p72d891894d)\" d=\"M 23.425 222.167464 \r\nL 240.865 222.167464 \r\n\" style=\"fill:none;stroke:#b2b2b2;stroke-dasharray:1.85,0.8;stroke-dashoffset:0;stroke-width:0.5;\"/>\r\n     </g>\r\n     <g id=\"line2d_24\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"23.425\" xlink:href=\"#m6a1bba78c1\" y=\"222.167464\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 225.966683)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 23.425 241.58175 \r\nL 23.425 24.14175 \r\n\" style=\"fill:none;stroke:#bcbcbc;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 240.865 241.58175 \r\nL 240.865 24.14175 \r\n\" style=\"fill:none;stroke:#bcbcbc;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 23.425 241.58175 \r\nL 240.865 241.58175 \r\n\" style=\"fill:none;stroke:#bcbcbc;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 23.425 24.14175 \r\nL 240.865 24.14175 \r\n\" style=\"fill:none;stroke:#bcbcbc;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_13\">\r\n    <!-- 9 -->\r\n    <g transform=\"translate(127.564 18.14175)scale(0.144 -0.144)\">\r\n     <defs>\r\n      <path d=\"M 703 97 \r\nL 703 672 \r\nQ 941 559 1184 500 \r\nQ 1428 441 1663 441 \r\nQ 2288 441 2617 861 \r\nQ 2947 1281 2994 2138 \r\nQ 2813 1869 2534 1725 \r\nQ 2256 1581 1919 1581 \r\nQ 1219 1581 811 2004 \r\nQ 403 2428 403 3163 \r\nQ 403 3881 828 4315 \r\nQ 1253 4750 1959 4750 \r\nQ 2769 4750 3195 4129 \r\nQ 3622 3509 3622 2328 \r\nQ 3622 1225 3098 567 \r\nQ 2575 -91 1691 -91 \r\nQ 1453 -91 1209 -44 \r\nQ 966 3 703 97 \r\nz\r\nM 1959 2075 \r\nQ 2384 2075 2632 2365 \r\nQ 2881 2656 2881 3163 \r\nQ 2881 3666 2632 3958 \r\nQ 2384 4250 1959 4250 \r\nQ 1534 4250 1286 3958 \r\nQ 1038 3666 1038 3163 \r\nQ 1038 2656 1286 2365 \r\nQ 1534 2075 1959 2075 \r\nz\r\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-39\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p72d891894d\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"23.425\" y=\"24.14175\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 0\n",
    "plt.imshow(train_data.train_data[idx].numpy(), cmap='GnBu')\n",
    "plt.title('%i' % train_data.train_labels[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, conv1_out=6):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              # input height\n",
    "                out_channels=conv1_out,     # n_filters\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # if want same width and length of this image after con2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (6, 28, 28)  28-5+1+2*2=28\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(kernel_size=2)     # choose max value in 2x2 area, output shape (6, 14, 14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         # input shape (6, 14, 14)\n",
    "            nn.Conv2d(conv1_out, 16, 5, 1, 0),      # output shape (16, 10, 10) 14-5+1+2*0=10\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(2)                 # output shape (16, 5, 5)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(         # input shape (16, 5, 5)\n",
    "            nn.Conv2d(16, 120, 5, 1, 0),    # output shape (120, 1, 1) 5-5+1+2*0=1\n",
    "            nn.ReLU()                       # activation\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(            # input shape (1, 5, 5)\n",
    "            nn.Linear(120, 84),             # output shape (84)\n",
    "            nn.ReLU(),                      # activation\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(            # input shape (1, 5, 5)\n",
    "            nn.Linear(84, 10),               # output shape (10)\n",
    "            nn.LogSoftmax(dim = -1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, **)\n",
    "        x = self.fc1(x)\n",
    "        output = self.fc2(x)\n",
    "\n",
    "        return output, x                    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(train_data=train_data, test_data=test_data, batch_size=50):\n",
    "    train_loader = Data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "    # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)\n",
    "    test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1)).type(torch.FloatTensor)/255.   \n",
    "    test_y = test_data.test_labels\n",
    "    ada_epoch = (batch_size * 20000) // len(train_data)\n",
    "    return train_loader, test_x.cuda(), test_y.cuda(), ada_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling(conv1_out=6, lr=0.001):\n",
    "    model = CNN(conv1_out=conv1_out)\n",
    "    model = model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)   # optimize all cnn parameters\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    loss_func = loss_func.cuda()                              # the target label is not one-hotted\n",
    "    return model, optimizer, loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing\n",
    "def train(train_loader, test_x, test_y, model, optimizer, loss_func, ada_epoch):\n",
    "    loss_arr = []\n",
    "    accu_arr = []\n",
    "    for epoch in range(ada_epoch):\n",
    "        for step, (x, y) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(x)   # batch x\n",
    "            b_y = Variable(y)   # batch y\n",
    "            if use_gpu:\n",
    "                b_x = b_x.cuda()\n",
    "                b_y = b_y.cuda()\n",
    "            output = model(b_x)[0]            # cnn output\n",
    "            loss = loss_func(output, b_y)   # cross entropy loss\n",
    "            optimizer.zero_grad()           # clear gradients for this training step\n",
    "            loss.backward()                 # backpropagation, compute gradients\n",
    "            optimizer.step()                # apply gradients\n",
    "            loss = loss.item()\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                test_output, last_layer = model(test_x)\n",
    "                pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "                accuracy = (pred_y == test_y).sum().item() / float(test_y.size(0))\n",
    "                loss_arr.append(loss)\n",
    "                accu_arr.append(accuracy)\n",
    "                print('Epoch: ', epoch, '| train loss: %.4f' % loss, '| test accuracy: %.4f' % accuracy)\n",
    "                \n",
    "    return loss_arr, accu_arr, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_plot(multi_loss_arr, multi_accu_arr, plot_name):\n",
    "    plt.style.use('bmh')\n",
    "    plt.figure(dpi=300)\n",
    "    plt.title('Training Process')    \n",
    "    for i in range(len(multi_accu_arr)):\n",
    "        loss_arr = multi_loss_arr[i]\n",
    "        accu_arr = multi_accu_arr[i]\n",
    "        plt.plot(range(len(loss_arr)), loss_arr, label='loss{}'.format(i),linewidth=0.7)\n",
    "        plt.plot(range(len(accu_arr)), accu_arr, label='accu{}'.format(i),linewidth=0.7)\n",
    "    plt.yticks([0.05*i for i in range(21)])\n",
    "    plt.ylim([0,1])\n",
    "    plt.legend()\n",
    "    plt.savefig('./pics/{}.jpg'.format(plot_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_batch(batch_size_arr, plot_name='test00'):\n",
    "    multi_loss_arr = []\n",
    "    multi_accu_arr = []\n",
    "    model_arr = []\n",
    "    for batch_size in batch_size_arr:\n",
    "        train_loader, test_x, test_y, ada_epoch = data_preparation(batch_size=batch_size)\n",
    "        model, optimizer, loss_func = modeling()\n",
    "        loss_arr, accu_arr, model = train(train_loader, test_x, test_y, model, optimizer, loss_func, ada_epoch)\n",
    "        multi_loss_arr.append(loss_arr)\n",
    "        multi_accu_arr.append(accu_arr)\n",
    "        model_arr.append(model)\n",
    "    training_plot(multi_loss_arr, multi_accu_arr, plot_name)\n",
    "    final_accu = multi_accu_arr[:][-1]\n",
    "    return model_arr, final_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_model_arr, batch_final_accu = exp_batch(batch_size_arr=[50,100,150],plot_name='batch_test(50,100,150)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_conv1_out(conv1_out_arr=[8,16,24], plot_name='test01'):\n",
    "    multi_loss_arr = []\n",
    "    multi_accu_arr = []\n",
    "    model_arr = []\n",
    "    for conv1_out in conv1_out_arr:\n",
    "        train_loader, test_x, test_y, ada_epoch = data_preparation(batch_size=100)\n",
    "        model, optimizer, loss_func = modeling(conv1_out=conv1_out)\n",
    "        loss_arr, accu_arr, model = train(train_loader, test_x, test_y, model, optimizer, loss_func, ada_epoch)\n",
    "        multi_loss_arr.append(loss_arr)\n",
    "        multi_accu_arr.append(accu_arr)\n",
    "        model_arr.append(model)\n",
    "    training_plot(multi_loss_arr, multi_accu_arr, plot_name)\n",
    "    final_accu = multi_accu_arr[:][-1]\n",
    "    return model_arr, final_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.3084 | test accuracy: 0.1000\n",
      "Epoch:  0 | train loss: 0.6544 | test accuracy: 0.7074\n",
      "Epoch:  0 | train loss: 0.9056 | test accuracy: 0.7427\n",
      "Epoch:  0 | train loss: 0.7237 | test accuracy: 0.7649\n",
      "Epoch:  0 | train loss: 0.4208 | test accuracy: 0.7770\n",
      "Epoch:  0 | train loss: 0.5354 | test accuracy: 0.8064\n",
      "Epoch:  1 | train loss: 0.5370 | test accuracy: 0.8098\n",
      "Epoch:  1 | train loss: 0.3241 | test accuracy: 0.8195\n",
      "Epoch:  1 | train loss: 0.4331 | test accuracy: 0.8199\n",
      "Epoch:  1 | train loss: 0.4612 | test accuracy: 0.8336\n",
      "Epoch:  1 | train loss: 0.4850 | test accuracy: 0.8367\n",
      "Epoch:  1 | train loss: 0.2905 | test accuracy: 0.8488\n",
      "Epoch:  2 | train loss: 0.5261 | test accuracy: 0.8552\n",
      "Epoch:  2 | train loss: 0.4602 | test accuracy: 0.8488\n",
      "Epoch:  2 | train loss: 0.4970 | test accuracy: 0.8602\n",
      "Epoch:  2 | train loss: 0.4922 | test accuracy: 0.8585\n",
      "Epoch:  2 | train loss: 0.4419 | test accuracy: 0.8645\n",
      "Epoch:  2 | train loss: 0.2914 | test accuracy: 0.8671\n",
      "Epoch:  3 | train loss: 0.3123 | test accuracy: 0.8588\n",
      "Epoch:  3 | train loss: 0.3587 | test accuracy: 0.8598\n",
      "Epoch:  3 | train loss: 0.3265 | test accuracy: 0.8715\n",
      "Epoch:  3 | train loss: 0.4781 | test accuracy: 0.8690\n",
      "Epoch:  3 | train loss: 0.4720 | test accuracy: 0.8732\n",
      "Epoch:  3 | train loss: 0.3644 | test accuracy: 0.8759\n",
      "Epoch:  4 | train loss: 0.3310 | test accuracy: 0.8694\n",
      "Epoch:  4 | train loss: 0.3285 | test accuracy: 0.8753\n",
      "Epoch:  4 | train loss: 0.2494 | test accuracy: 0.8802\n",
      "Epoch:  4 | train loss: 0.1814 | test accuracy: 0.8733\n",
      "Epoch:  4 | train loss: 0.3286 | test accuracy: 0.8757\n",
      "Epoch:  4 | train loss: 0.2956 | test accuracy: 0.8754\n",
      "Epoch:  5 | train loss: 0.3143 | test accuracy: 0.8771\n",
      "Epoch:  5 | train loss: 0.2100 | test accuracy: 0.8802\n",
      "Epoch:  5 | train loss: 0.3286 | test accuracy: 0.8793\n",
      "Epoch:  5 | train loss: 0.2436 | test accuracy: 0.8780\n",
      "Epoch:  5 | train loss: 0.2839 | test accuracy: 0.8857\n",
      "Epoch:  5 | train loss: 0.2502 | test accuracy: 0.8880\n",
      "Epoch:  6 | train loss: 0.2549 | test accuracy: 0.8862\n",
      "Epoch:  6 | train loss: 0.2826 | test accuracy: 0.8865\n",
      "Epoch:  6 | train loss: 0.3432 | test accuracy: 0.8775\n",
      "Epoch:  6 | train loss: 0.3899 | test accuracy: 0.8787\n",
      "Epoch:  6 | train loss: 0.2480 | test accuracy: 0.8862\n",
      "Epoch:  6 | train loss: 0.3923 | test accuracy: 0.8894\n",
      "Epoch:  7 | train loss: 0.2479 | test accuracy: 0.8922\n",
      "Epoch:  7 | train loss: 0.3814 | test accuracy: 0.8875\n",
      "Epoch:  7 | train loss: 0.2016 | test accuracy: 0.8890\n",
      "Epoch:  7 | train loss: 0.2185 | test accuracy: 0.8958\n",
      "Epoch:  7 | train loss: 0.2434 | test accuracy: 0.8894\n",
      "Epoch:  7 | train loss: 0.2158 | test accuracy: 0.8877\n",
      "Epoch:  8 | train loss: 0.2068 | test accuracy: 0.8918\n",
      "Epoch:  8 | train loss: 0.1731 | test accuracy: 0.8903\n",
      "Epoch:  8 | train loss: 0.3302 | test accuracy: 0.8922\n",
      "Epoch:  8 | train loss: 0.2270 | test accuracy: 0.8862\n",
      "Epoch:  8 | train loss: 0.2381 | test accuracy: 0.8923\n",
      "Epoch:  8 | train loss: 0.1841 | test accuracy: 0.8954\n",
      "Epoch:  9 | train loss: 0.2399 | test accuracy: 0.8924\n",
      "Epoch:  9 | train loss: 0.2407 | test accuracy: 0.8956\n",
      "Epoch:  9 | train loss: 0.1879 | test accuracy: 0.8950\n",
      "Epoch:  9 | train loss: 0.4100 | test accuracy: 0.8954\n",
      "Epoch:  9 | train loss: 0.2101 | test accuracy: 0.8960\n",
      "Epoch:  9 | train loss: 0.2066 | test accuracy: 0.8848\n",
      "Epoch:  10 | train loss: 0.1645 | test accuracy: 0.8964\n",
      "Epoch:  10 | train loss: 0.1810 | test accuracy: 0.8958\n",
      "Epoch:  10 | train loss: 0.2853 | test accuracy: 0.9004\n",
      "Epoch:  10 | train loss: 0.1331 | test accuracy: 0.8985\n",
      "Epoch:  10 | train loss: 0.1901 | test accuracy: 0.8972\n",
      "Epoch:  10 | train loss: 0.2077 | test accuracy: 0.8991\n",
      "Epoch:  11 | train loss: 0.1496 | test accuracy: 0.8981\n",
      "Epoch:  11 | train loss: 0.2143 | test accuracy: 0.8904\n",
      "Epoch:  11 | train loss: 0.2246 | test accuracy: 0.9034\n",
      "Epoch:  11 | train loss: 0.2638 | test accuracy: 0.8999\n",
      "Epoch:  11 | train loss: 0.2653 | test accuracy: 0.8992\n",
      "Epoch:  11 | train loss: 0.2080 | test accuracy: 0.8971\n",
      "Epoch:  12 | train loss: 0.2693 | test accuracy: 0.9007\n",
      "Epoch:  12 | train loss: 0.1832 | test accuracy: 0.8962\n",
      "Epoch:  12 | train loss: 0.1952 | test accuracy: 0.9012\n",
      "Epoch:  12 | train loss: 0.1556 | test accuracy: 0.8993\n",
      "Epoch:  12 | train loss: 0.2081 | test accuracy: 0.9011\n",
      "Epoch:  12 | train loss: 0.2113 | test accuracy: 0.8997\n",
      "Epoch:  13 | train loss: 0.1274 | test accuracy: 0.9002\n",
      "Epoch:  13 | train loss: 0.1437 | test accuracy: 0.8972\n",
      "Epoch:  13 | train loss: 0.3270 | test accuracy: 0.8988\n",
      "Epoch:  13 | train loss: 0.1846 | test accuracy: 0.9044\n",
      "Epoch:  13 | train loss: 0.1938 | test accuracy: 0.8991\n",
      "Epoch:  13 | train loss: 0.2053 | test accuracy: 0.9024\n",
      "Epoch:  14 | train loss: 0.1138 | test accuracy: 0.9025\n",
      "Epoch:  14 | train loss: 0.1704 | test accuracy: 0.9032\n",
      "Epoch:  14 | train loss: 0.2366 | test accuracy: 0.9033\n",
      "Epoch:  14 | train loss: 0.1664 | test accuracy: 0.9029\n",
      "Epoch:  14 | train loss: 0.1315 | test accuracy: 0.9035\n",
      "Epoch:  14 | train loss: 0.1034 | test accuracy: 0.9035\n",
      "Epoch:  15 | train loss: 0.2312 | test accuracy: 0.9011\n",
      "Epoch:  15 | train loss: 0.1937 | test accuracy: 0.9029\n",
      "Epoch:  15 | train loss: 0.2646 | test accuracy: 0.9073\n",
      "Epoch:  15 | train loss: 0.1579 | test accuracy: 0.9043\n",
      "Epoch:  15 | train loss: 0.1746 | test accuracy: 0.9036\n",
      "Epoch:  15 | train loss: 0.2732 | test accuracy: 0.9078\n",
      "Epoch:  16 | train loss: 0.1752 | test accuracy: 0.9045\n",
      "Epoch:  16 | train loss: 0.1633 | test accuracy: 0.9031\n",
      "Epoch:  16 | train loss: 0.1548 | test accuracy: 0.9048\n",
      "Epoch:  16 | train loss: 0.1410 | test accuracy: 0.9009\n",
      "Epoch:  16 | train loss: 0.2454 | test accuracy: 0.9073\n",
      "Epoch:  16 | train loss: 0.1205 | test accuracy: 0.9072\n",
      "Epoch:  17 | train loss: 0.1413 | test accuracy: 0.9068\n",
      "Epoch:  17 | train loss: 0.1183 | test accuracy: 0.9006\n",
      "Epoch:  17 | train loss: 0.2375 | test accuracy: 0.9022\n",
      "Epoch:  17 | train loss: 0.0820 | test accuracy: 0.9014\n",
      "Epoch:  17 | train loss: 0.1003 | test accuracy: 0.9047\n",
      "Epoch:  17 | train loss: 0.1449 | test accuracy: 0.9048\n",
      "Epoch:  18 | train loss: 0.1675 | test accuracy: 0.9040\n",
      "Epoch:  18 | train loss: 0.1911 | test accuracy: 0.9007\n",
      "Epoch:  18 | train loss: 0.1432 | test accuracy: 0.9067\n",
      "Epoch:  18 | train loss: 0.2972 | test accuracy: 0.9034\n",
      "Epoch:  18 | train loss: 0.0666 | test accuracy: 0.8990\n",
      "Epoch:  18 | train loss: 0.2416 | test accuracy: 0.9064\n",
      "Epoch:  19 | train loss: 0.2039 | test accuracy: 0.9040\n",
      "Epoch:  19 | train loss: 0.1493 | test accuracy: 0.9057\n",
      "Epoch:  19 | train loss: 0.1878 | test accuracy: 0.9059\n",
      "Epoch:  19 | train loss: 0.1261 | test accuracy: 0.9076\n",
      "Epoch:  19 | train loss: 0.0942 | test accuracy: 0.9045\n",
      "Epoch:  19 | train loss: 0.1320 | test accuracy: 0.9084\n",
      "Epoch:  20 | train loss: 0.1643 | test accuracy: 0.9014\n",
      "Epoch:  20 | train loss: 0.1257 | test accuracy: 0.9074\n",
      "Epoch:  20 | train loss: 0.2409 | test accuracy: 0.9063\n",
      "Epoch:  20 | train loss: 0.2043 | test accuracy: 0.8999\n",
      "Epoch:  20 | train loss: 0.1022 | test accuracy: 0.9046\n",
      "Epoch:  20 | train loss: 0.1040 | test accuracy: 0.9059\n",
      "Epoch:  21 | train loss: 0.0992 | test accuracy: 0.9046\n",
      "Epoch:  21 | train loss: 0.1941 | test accuracy: 0.9053\n",
      "Epoch:  21 | train loss: 0.1119 | test accuracy: 0.9087\n",
      "Epoch:  21 | train loss: 0.1256 | test accuracy: 0.9077\n",
      "Epoch:  21 | train loss: 0.0922 | test accuracy: 0.8965\n",
      "Epoch:  21 | train loss: 0.1391 | test accuracy: 0.9062\n",
      "Epoch:  22 | train loss: 0.0817 | test accuracy: 0.9052\n",
      "Epoch:  22 | train loss: 0.1369 | test accuracy: 0.9078\n",
      "Epoch:  22 | train loss: 0.0787 | test accuracy: 0.9041\n",
      "Epoch:  22 | train loss: 0.1020 | test accuracy: 0.9059\n",
      "Epoch:  22 | train loss: 0.1496 | test accuracy: 0.9054\n",
      "Epoch:  22 | train loss: 0.1216 | test accuracy: 0.8997\n",
      "Epoch:  23 | train loss: 0.1165 | test accuracy: 0.9051\n",
      "Epoch:  23 | train loss: 0.1587 | test accuracy: 0.9049\n",
      "Epoch:  23 | train loss: 0.1063 | test accuracy: 0.9024\n",
      "Epoch:  23 | train loss: 0.1344 | test accuracy: 0.9063\n",
      "Epoch:  23 | train loss: 0.0969 | test accuracy: 0.9051\n",
      "Epoch:  23 | train loss: 0.1272 | test accuracy: 0.9109\n",
      "Epoch:  24 | train loss: 0.0835 | test accuracy: 0.9066\n",
      "Epoch:  24 | train loss: 0.2083 | test accuracy: 0.9025\n",
      "Epoch:  24 | train loss: 0.1854 | test accuracy: 0.9065\n",
      "Epoch:  24 | train loss: 0.0953 | test accuracy: 0.9055\n",
      "Epoch:  24 | train loss: 0.2134 | test accuracy: 0.9051\n",
      "Epoch:  24 | train loss: 0.1347 | test accuracy: 0.9063\n",
      "Epoch:  25 | train loss: 0.0610 | test accuracy: 0.9073\n",
      "Epoch:  25 | train loss: 0.1548 | test accuracy: 0.9069\n",
      "Epoch:  25 | train loss: 0.0561 | test accuracy: 0.9067\n",
      "Epoch:  25 | train loss: 0.2795 | test accuracy: 0.8995\n",
      "Epoch:  25 | train loss: 0.1059 | test accuracy: 0.9054\n",
      "Epoch:  25 | train loss: 0.0832 | test accuracy: 0.9046\n",
      "Epoch:  26 | train loss: 0.2425 | test accuracy: 0.9066\n",
      "Epoch:  26 | train loss: 0.0573 | test accuracy: 0.9073\n",
      "Epoch:  26 | train loss: 0.1272 | test accuracy: 0.9032\n",
      "Epoch:  26 | train loss: 0.1701 | test accuracy: 0.9041\n",
      "Epoch:  26 | train loss: 0.0869 | test accuracy: 0.9045\n",
      "Epoch:  26 | train loss: 0.1467 | test accuracy: 0.9061\n",
      "Epoch:  27 | train loss: 0.0790 | test accuracy: 0.9073\n",
      "Epoch:  27 | train loss: 0.0886 | test accuracy: 0.9039\n",
      "Epoch:  27 | train loss: 0.1121 | test accuracy: 0.9044\n",
      "Epoch:  27 | train loss: 0.0652 | test accuracy: 0.9064\n",
      "Epoch:  27 | train loss: 0.1356 | test accuracy: 0.9055\n",
      "Epoch:  27 | train loss: 0.1162 | test accuracy: 0.9028\n",
      "Epoch:  28 | train loss: 0.1305 | test accuracy: 0.9057\n",
      "Epoch:  28 | train loss: 0.1488 | test accuracy: 0.9027\n",
      "Epoch:  28 | train loss: 0.1084 | test accuracy: 0.9046\n",
      "Epoch:  28 | train loss: 0.1561 | test accuracy: 0.9060\n",
      "Epoch:  28 | train loss: 0.2301 | test accuracy: 0.9027\n",
      "Epoch:  28 | train loss: 0.1044 | test accuracy: 0.9028\n",
      "Epoch:  29 | train loss: 0.1030 | test accuracy: 0.9010\n",
      "Epoch:  29 | train loss: 0.0918 | test accuracy: 0.9042\n",
      "Epoch:  29 | train loss: 0.0961 | test accuracy: 0.9017\n",
      "Epoch:  29 | train loss: 0.1303 | test accuracy: 0.9074\n",
      "Epoch:  29 | train loss: 0.0955 | test accuracy: 0.9024\n",
      "Epoch:  29 | train loss: 0.1085 | test accuracy: 0.9044\n",
      "Epoch:  30 | train loss: 0.0798 | test accuracy: 0.9007\n",
      "Epoch:  30 | train loss: 0.1545 | test accuracy: 0.8993\n",
      "Epoch:  30 | train loss: 0.1007 | test accuracy: 0.9066\n",
      "Epoch:  30 | train loss: 0.0990 | test accuracy: 0.9040\n",
      "Epoch:  30 | train loss: 0.0741 | test accuracy: 0.9041\n",
      "Epoch:  30 | train loss: 0.0702 | test accuracy: 0.8974\n",
      "Epoch:  31 | train loss: 0.0744 | test accuracy: 0.9035\n",
      "Epoch:  31 | train loss: 0.0461 | test accuracy: 0.9022\n",
      "Epoch:  31 | train loss: 0.1234 | test accuracy: 0.9054\n",
      "Epoch:  31 | train loss: 0.0880 | test accuracy: 0.9037\n",
      "Epoch:  31 | train loss: 0.0715 | test accuracy: 0.9045\n",
      "Epoch:  31 | train loss: 0.0384 | test accuracy: 0.9051\n",
      "Epoch:  32 | train loss: 0.1274 | test accuracy: 0.8877\n",
      "Epoch:  32 | train loss: 0.0683 | test accuracy: 0.9022\n",
      "Epoch:  32 | train loss: 0.0740 | test accuracy: 0.9064\n",
      "Epoch:  32 | train loss: 0.0419 | test accuracy: 0.9071\n",
      "Epoch:  32 | train loss: 0.1059 | test accuracy: 0.9019\n",
      "Epoch:  32 | train loss: 0.0981 | test accuracy: 0.9048\n",
      "Epoch:  0 | train loss: 2.3067 | test accuracy: 0.1000\n",
      "Epoch:  0 | train loss: 0.7880 | test accuracy: 0.6730\n",
      "Epoch:  0 | train loss: 0.5902 | test accuracy: 0.7414\n",
      "Epoch:  0 | train loss: 0.4582 | test accuracy: 0.7781\n",
      "Epoch:  0 | train loss: 0.3899 | test accuracy: 0.7822\n",
      "Epoch:  0 | train loss: 0.4300 | test accuracy: 0.7921\n",
      "Epoch:  1 | train loss: 0.5492 | test accuracy: 0.8137\n",
      "Epoch:  1 | train loss: 0.4630 | test accuracy: 0.8274\n",
      "Epoch:  1 | train loss: 0.5498 | test accuracy: 0.8198\n",
      "Epoch:  1 | train loss: 0.3027 | test accuracy: 0.8376\n",
      "Epoch:  1 | train loss: 0.4175 | test accuracy: 0.8476\n",
      "Epoch:  1 | train loss: 0.2966 | test accuracy: 0.8391\n",
      "Epoch:  2 | train loss: 0.5171 | test accuracy: 0.8484\n",
      "Epoch:  2 | train loss: 0.4153 | test accuracy: 0.8244\n",
      "Epoch:  2 | train loss: 0.4630 | test accuracy: 0.8415\n",
      "Epoch:  2 | train loss: 0.2635 | test accuracy: 0.8635\n",
      "Epoch:  2 | train loss: 0.3264 | test accuracy: 0.8577\n",
      "Epoch:  2 | train loss: 0.2303 | test accuracy: 0.8658\n",
      "Epoch:  3 | train loss: 0.3283 | test accuracy: 0.8638\n",
      "Epoch:  3 | train loss: 0.4530 | test accuracy: 0.8588\n",
      "Epoch:  3 | train loss: 0.3206 | test accuracy: 0.8715\n",
      "Epoch:  3 | train loss: 0.3624 | test accuracy: 0.8675\n",
      "Epoch:  3 | train loss: 0.3779 | test accuracy: 0.8684\n",
      "Epoch:  3 | train loss: 0.4297 | test accuracy: 0.8694\n",
      "Epoch:  4 | train loss: 0.3876 | test accuracy: 0.8707\n",
      "Epoch:  4 | train loss: 0.3664 | test accuracy: 0.8741\n",
      "Epoch:  4 | train loss: 0.4161 | test accuracy: 0.8716\n",
      "Epoch:  4 | train loss: 0.3273 | test accuracy: 0.8739\n",
      "Epoch:  4 | train loss: 0.4848 | test accuracy: 0.8765\n",
      "Epoch:  4 | train loss: 0.3895 | test accuracy: 0.8715\n",
      "Epoch:  5 | train loss: 0.2598 | test accuracy: 0.8806\n",
      "Epoch:  5 | train loss: 0.2412 | test accuracy: 0.8807\n",
      "Epoch:  5 | train loss: 0.3233 | test accuracy: 0.8756\n",
      "Epoch:  5 | train loss: 0.5205 | test accuracy: 0.8791\n",
      "Epoch:  5 | train loss: 0.4668 | test accuracy: 0.8792\n",
      "Epoch:  5 | train loss: 0.3371 | test accuracy: 0.8819\n",
      "Epoch:  6 | train loss: 0.3055 | test accuracy: 0.8836\n",
      "Epoch:  6 | train loss: 0.2880 | test accuracy: 0.8848\n",
      "Epoch:  6 | train loss: 0.1984 | test accuracy: 0.8873\n",
      "Epoch:  6 | train loss: 0.2967 | test accuracy: 0.8891\n",
      "Epoch:  6 | train loss: 0.2279 | test accuracy: 0.8814\n",
      "Epoch:  6 | train loss: 0.2020 | test accuracy: 0.8877\n",
      "Epoch:  7 | train loss: 0.1948 | test accuracy: 0.8902\n",
      "Epoch:  7 | train loss: 0.2619 | test accuracy: 0.8851\n",
      "Epoch:  7 | train loss: 0.3156 | test accuracy: 0.8909\n",
      "Epoch:  7 | train loss: 0.3580 | test accuracy: 0.8889\n",
      "Epoch:  7 | train loss: 0.3250 | test accuracy: 0.8860\n",
      "Epoch:  7 | train loss: 0.3554 | test accuracy: 0.8866\n",
      "Epoch:  8 | train loss: 0.4245 | test accuracy: 0.8935\n",
      "Epoch:  8 | train loss: 0.2681 | test accuracy: 0.8969\n",
      "Epoch:  8 | train loss: 0.4150 | test accuracy: 0.8956\n",
      "Epoch:  8 | train loss: 0.2436 | test accuracy: 0.8891\n",
      "Epoch:  8 | train loss: 0.1975 | test accuracy: 0.8933\n",
      "Epoch:  8 | train loss: 0.1439 | test accuracy: 0.8951\n",
      "Epoch:  9 | train loss: 0.2335 | test accuracy: 0.8993\n",
      "Epoch:  9 | train loss: 0.2602 | test accuracy: 0.8965\n",
      "Epoch:  9 | train loss: 0.1918 | test accuracy: 0.8965\n",
      "Epoch:  9 | train loss: 0.2877 | test accuracy: 0.8961\n",
      "Epoch:  9 | train loss: 0.3077 | test accuracy: 0.8886\n",
      "Epoch:  9 | train loss: 0.2659 | test accuracy: 0.8946\n",
      "Epoch:  10 | train loss: 0.2887 | test accuracy: 0.8982\n",
      "Epoch:  10 | train loss: 0.2272 | test accuracy: 0.8987\n",
      "Epoch:  10 | train loss: 0.2473 | test accuracy: 0.8912\n",
      "Epoch:  10 | train loss: 0.2171 | test accuracy: 0.8941\n",
      "Epoch:  10 | train loss: 0.2176 | test accuracy: 0.8956\n",
      "Epoch:  10 | train loss: 0.2174 | test accuracy: 0.8973\n",
      "Epoch:  11 | train loss: 0.2036 | test accuracy: 0.8979\n",
      "Epoch:  11 | train loss: 0.1888 | test accuracy: 0.8950\n",
      "Epoch:  11 | train loss: 0.1789 | test accuracy: 0.8936\n",
      "Epoch:  11 | train loss: 0.1353 | test accuracy: 0.8994\n",
      "Epoch:  11 | train loss: 0.2482 | test accuracy: 0.8984\n",
      "Epoch:  11 | train loss: 0.1904 | test accuracy: 0.8946\n",
      "Epoch:  12 | train loss: 0.1736 | test accuracy: 0.8980\n",
      "Epoch:  12 | train loss: 0.1166 | test accuracy: 0.9008\n",
      "Epoch:  12 | train loss: 0.1415 | test accuracy: 0.8945\n",
      "Epoch:  12 | train loss: 0.2420 | test accuracy: 0.9005\n",
      "Epoch:  12 | train loss: 0.3722 | test accuracy: 0.8996\n",
      "Epoch:  12 | train loss: 0.1623 | test accuracy: 0.8971\n",
      "Epoch:  13 | train loss: 0.0985 | test accuracy: 0.8972\n",
      "Epoch:  13 | train loss: 0.2436 | test accuracy: 0.9026\n",
      "Epoch:  13 | train loss: 0.1755 | test accuracy: 0.9010\n",
      "Epoch:  13 | train loss: 0.2999 | test accuracy: 0.9010\n",
      "Epoch:  13 | train loss: 0.0799 | test accuracy: 0.9002\n",
      "Epoch:  13 | train loss: 0.3289 | test accuracy: 0.9028\n",
      "Epoch:  14 | train loss: 0.2562 | test accuracy: 0.9001\n",
      "Epoch:  14 | train loss: 0.1983 | test accuracy: 0.9010\n",
      "Epoch:  14 | train loss: 0.2309 | test accuracy: 0.9004\n",
      "Epoch:  14 | train loss: 0.2682 | test accuracy: 0.8965\n",
      "Epoch:  14 | train loss: 0.2943 | test accuracy: 0.9022\n",
      "Epoch:  14 | train loss: 0.1230 | test accuracy: 0.9014\n",
      "Epoch:  15 | train loss: 0.1600 | test accuracy: 0.9050\n",
      "Epoch:  15 | train loss: 0.2060 | test accuracy: 0.9035\n",
      "Epoch:  15 | train loss: 0.2024 | test accuracy: 0.9029\n",
      "Epoch:  15 | train loss: 0.1363 | test accuracy: 0.8986\n",
      "Epoch:  15 | train loss: 0.1668 | test accuracy: 0.9003\n",
      "Epoch:  15 | train loss: 0.2063 | test accuracy: 0.9030\n",
      "Epoch:  16 | train loss: 0.1066 | test accuracy: 0.9033\n",
      "Epoch:  16 | train loss: 0.2612 | test accuracy: 0.9021\n",
      "Epoch:  16 | train loss: 0.1059 | test accuracy: 0.9030\n",
      "Epoch:  16 | train loss: 0.2569 | test accuracy: 0.9025\n",
      "Epoch:  16 | train loss: 0.1800 | test accuracy: 0.9023\n",
      "Epoch:  16 | train loss: 0.1295 | test accuracy: 0.9052\n",
      "Epoch:  17 | train loss: 0.1090 | test accuracy: 0.9042\n",
      "Epoch:  17 | train loss: 0.1172 | test accuracy: 0.9054\n",
      "Epoch:  17 | train loss: 0.1536 | test accuracy: 0.8973\n",
      "Epoch:  17 | train loss: 0.3085 | test accuracy: 0.9006\n",
      "Epoch:  17 | train loss: 0.2009 | test accuracy: 0.8962\n",
      "Epoch:  17 | train loss: 0.1133 | test accuracy: 0.9002\n",
      "Epoch:  18 | train loss: 0.1657 | test accuracy: 0.8988\n",
      "Epoch:  18 | train loss: 0.1171 | test accuracy: 0.9064\n",
      "Epoch:  18 | train loss: 0.1246 | test accuracy: 0.8990\n",
      "Epoch:  18 | train loss: 0.0835 | test accuracy: 0.9022\n",
      "Epoch:  18 | train loss: 0.1982 | test accuracy: 0.8963\n",
      "Epoch:  18 | train loss: 0.2386 | test accuracy: 0.9017\n",
      "Epoch:  19 | train loss: 0.1193 | test accuracy: 0.8983\n",
      "Epoch:  19 | train loss: 0.1265 | test accuracy: 0.9036\n",
      "Epoch:  19 | train loss: 0.1498 | test accuracy: 0.9030\n",
      "Epoch:  19 | train loss: 0.1612 | test accuracy: 0.9004\n",
      "Epoch:  19 | train loss: 0.1243 | test accuracy: 0.9032\n",
      "Epoch:  19 | train loss: 0.1119 | test accuracy: 0.8938\n",
      "Epoch:  20 | train loss: 0.1110 | test accuracy: 0.9029\n",
      "Epoch:  20 | train loss: 0.0926 | test accuracy: 0.9055\n",
      "Epoch:  20 | train loss: 0.1689 | test accuracy: 0.9012\n",
      "Epoch:  20 | train loss: 0.1496 | test accuracy: 0.9024\n",
      "Epoch:  20 | train loss: 0.1599 | test accuracy: 0.8951\n",
      "Epoch:  20 | train loss: 0.1322 | test accuracy: 0.9058\n",
      "Epoch:  21 | train loss: 0.1052 | test accuracy: 0.8967\n",
      "Epoch:  21 | train loss: 0.1296 | test accuracy: 0.9017\n",
      "Epoch:  21 | train loss: 0.1515 | test accuracy: 0.9038\n",
      "Epoch:  21 | train loss: 0.0934 | test accuracy: 0.9061\n",
      "Epoch:  21 | train loss: 0.1562 | test accuracy: 0.8996\n",
      "Epoch:  21 | train loss: 0.1108 | test accuracy: 0.9058\n",
      "Epoch:  22 | train loss: 0.1720 | test accuracy: 0.9073\n",
      "Epoch:  22 | train loss: 0.1362 | test accuracy: 0.9046\n",
      "Epoch:  22 | train loss: 0.0845 | test accuracy: 0.9052\n",
      "Epoch:  22 | train loss: 0.0978 | test accuracy: 0.9002\n",
      "Epoch:  22 | train loss: 0.1281 | test accuracy: 0.9006\n",
      "Epoch:  22 | train loss: 0.1583 | test accuracy: 0.9010\n",
      "Epoch:  23 | train loss: 0.2141 | test accuracy: 0.9039\n",
      "Epoch:  23 | train loss: 0.1378 | test accuracy: 0.9041\n",
      "Epoch:  23 | train loss: 0.1103 | test accuracy: 0.9032\n",
      "Epoch:  23 | train loss: 0.1540 | test accuracy: 0.8993\n",
      "Epoch:  23 | train loss: 0.1949 | test accuracy: 0.8974\n",
      "Epoch:  23 | train loss: 0.1606 | test accuracy: 0.9025\n",
      "Epoch:  24 | train loss: 0.1150 | test accuracy: 0.9027\n",
      "Epoch:  24 | train loss: 0.0786 | test accuracy: 0.9004\n",
      "Epoch:  24 | train loss: 0.1181 | test accuracy: 0.9032\n",
      "Epoch:  24 | train loss: 0.1270 | test accuracy: 0.9016\n",
      "Epoch:  24 | train loss: 0.0938 | test accuracy: 0.9009\n",
      "Epoch:  24 | train loss: 0.1256 | test accuracy: 0.9016\n",
      "Epoch:  25 | train loss: 0.0456 | test accuracy: 0.9002\n",
      "Epoch:  25 | train loss: 0.0612 | test accuracy: 0.9021\n",
      "Epoch:  25 | train loss: 0.1277 | test accuracy: 0.9024\n",
      "Epoch:  25 | train loss: 0.1026 | test accuracy: 0.9013\n",
      "Epoch:  25 | train loss: 0.1824 | test accuracy: 0.8984\n",
      "Epoch:  25 | train loss: 0.1649 | test accuracy: 0.9032\n",
      "Epoch:  26 | train loss: 0.0810 | test accuracy: 0.9063\n",
      "Epoch:  26 | train loss: 0.1205 | test accuracy: 0.9022\n",
      "Epoch:  26 | train loss: 0.0927 | test accuracy: 0.9059\n",
      "Epoch:  26 | train loss: 0.1158 | test accuracy: 0.9009\n",
      "Epoch:  26 | train loss: 0.0832 | test accuracy: 0.9039\n",
      "Epoch:  26 | train loss: 0.1854 | test accuracy: 0.9024\n",
      "Epoch:  27 | train loss: 0.1212 | test accuracy: 0.9029\n",
      "Epoch:  27 | train loss: 0.1469 | test accuracy: 0.9047\n",
      "Epoch:  27 | train loss: 0.1086 | test accuracy: 0.9040\n",
      "Epoch:  27 | train loss: 0.0630 | test accuracy: 0.9026\n",
      "Epoch:  27 | train loss: 0.1706 | test accuracy: 0.9042\n",
      "Epoch:  27 | train loss: 0.1479 | test accuracy: 0.8999\n",
      "Epoch:  28 | train loss: 0.0470 | test accuracy: 0.8949\n",
      "Epoch:  28 | train loss: 0.0552 | test accuracy: 0.9046\n",
      "Epoch:  28 | train loss: 0.0719 | test accuracy: 0.9031\n",
      "Epoch:  28 | train loss: 0.0959 | test accuracy: 0.9011\n",
      "Epoch:  28 | train loss: 0.1131 | test accuracy: 0.9001\n",
      "Epoch:  28 | train loss: 0.0863 | test accuracy: 0.9008\n",
      "Epoch:  29 | train loss: 0.1132 | test accuracy: 0.9012\n",
      "Epoch:  29 | train loss: 0.1027 | test accuracy: 0.9036\n",
      "Epoch:  29 | train loss: 0.1509 | test accuracy: 0.9042\n",
      "Epoch:  29 | train loss: 0.0744 | test accuracy: 0.9028\n",
      "Epoch:  29 | train loss: 0.0921 | test accuracy: 0.8994\n",
      "Epoch:  29 | train loss: 0.0505 | test accuracy: 0.9017\n",
      "Epoch:  30 | train loss: 0.0931 | test accuracy: 0.8998\n",
      "Epoch:  30 | train loss: 0.1514 | test accuracy: 0.9061\n",
      "Epoch:  30 | train loss: 0.1161 | test accuracy: 0.9029\n",
      "Epoch:  30 | train loss: 0.0436 | test accuracy: 0.9004\n",
      "Epoch:  30 | train loss: 0.1542 | test accuracy: 0.8994\n",
      "Epoch:  30 | train loss: 0.0740 | test accuracy: 0.9024\n",
      "Epoch:  31 | train loss: 0.1308 | test accuracy: 0.9020\n",
      "Epoch:  31 | train loss: 0.0578 | test accuracy: 0.9003\n",
      "Epoch:  31 | train loss: 0.0910 | test accuracy: 0.9003\n",
      "Epoch:  31 | train loss: 0.1504 | test accuracy: 0.9032\n",
      "Epoch:  31 | train loss: 0.1229 | test accuracy: 0.8988\n",
      "Epoch:  31 | train loss: 0.0888 | test accuracy: 0.8997\n",
      "Epoch:  32 | train loss: 0.0524 | test accuracy: 0.9025\n",
      "Epoch:  32 | train loss: 0.0693 | test accuracy: 0.9010\n",
      "Epoch:  32 | train loss: 0.1146 | test accuracy: 0.9017\n",
      "Epoch:  32 | train loss: 0.1986 | test accuracy: 0.9011\n",
      "Epoch:  32 | train loss: 0.0966 | test accuracy: 0.8944\n",
      "Epoch:  32 | train loss: 0.1300 | test accuracy: 0.9001\n",
      "Epoch:  0 | train loss: 2.3063 | test accuracy: 0.1892\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 958.00 MiB (GPU 0; 6.00 GiB total capacity; 3.55 GiB already allocated; 172.88 MiB free; 3.58 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SEELEA~1\\AppData\\Local\\Temp/ipykernel_8936/1210448873.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconv1out_model_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcon1out_final_accu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_conv1_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv1_out_arr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'conv1_test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\SEELEA~1\\AppData\\Local\\Temp/ipykernel_8936/193772392.py\u001b[0m in \u001b[0;36mexp_conv1_out\u001b[1;34m(conv1_out_arr, plot_name)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mada_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_preparation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv1_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconv1_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mloss_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccu_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mada_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mmulti_loss_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mmulti_accu_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccu_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SEELEA~1\\AppData\\Local\\Temp/ipykernel_8936/4585998.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, test_x, test_y, model, optimizer, loss_func, ada_epoch)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0mtest_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                 \u001b[0mpred_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpred_y\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SEELEA~1\\AppData\\Local\\Temp/ipykernel_8936/3095765184.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 439\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 958.00 MiB (GPU 0; 6.00 GiB total capacity; 3.55 GiB already allocated; 172.88 MiB free; 3.58 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "conv1out_model_arr, con1out_final_accu = exp_conv1_out(conv1_out_arr=[8,6,32], plot_name='conv1_test')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc389f016b4508cd8daa88f4ee394fd1693d7bd89433f66362fdc17b674936ae"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
